{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pramodgangula19/5731_Spring24/blob/main/Gangula_pramod_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question for Research: The study's goal is to determine how the use of electric vehicles (EVs) in a specific urban region affects air quality and to analyze the health and environmental repercussions.\n",
        "\n",
        "\n",
        "\n",
        "Steps for Data Collection:\n",
        "\n",
        "\n",
        "\n",
        "1. Create Variables: Determine critical variables such as EV adoption, air quality, health, and environmental data.\n",
        "\n",
        "2. Sample Selection: Select a specified urban region and specify the timeframe for the analysis.\n",
        "\n",
        "3. Sources of Information: Data should be obtained from government agencies, healthcare institutions, environmental agencies, automobile manufacturers, charging network providers, and meteorological agencies.\n",
        "\n",
        "4. Data Cleaning and Integration: Clean and preprocess data, deal with missing values, and combine data from several sources into a cohesive dataset.\n",
        "\n",
        "5. Data Analysis: Using statistical approaches, investigate the relationship between EV adoption and air quality, as well as the health and environmental consequences.\n",
        "\n",
        "6. Visualization: Create graphics to effectively present findings.\n",
        "\n",
        "7. Interpretation: Draw conclusions on the effect of EV adoption on air quality and its implications for health and the environment.\n",
        "\n",
        "8. Report and Publication: Document findings in a report or scholarly article, highlighting policy recommendations and the implications for urban sustainability.\n",
        "\n",
        "\n",
        "Sample Size: Select a sample size depending on statistical power requirements, taking into account multiple urban regions or time periods.\n",
        "\n",
        "\n",
        "Ethical Considerations: Make certain that data collection and analysis follow ethical norms, such as data privacy, informed permission, and coordination with appropriate authorities.\n",
        "\n",
        "\n",
        "The goal of this study is to provide insights into the effects of EV adoption on urban air quality and to contribute to informed urban planning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "# Import necessary libraries for data collection and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Simulated data sources (replace with actual data sources)\n",
        "# For illustration purposes, we'll create synthetic data here.\n",
        "# In a real scenario, you would access real data from various sources.\n",
        "\n",
        "# Generate EV adoption data (number of EVs in each year)\n",
        "years = range(2015, 2022)\n",
        "ev_adoption_data = {\n",
        "    \"Year\": years,\n",
        "    \"Number_of_EVs\": [1000, 1500, 2200, 3000, 4500, 6500, 9000]  # Replace with actual data\n",
        "}\n",
        "\n",
        "# Generate air quality data (PM2.5 concentrations in µg/m³)\n",
        "air_quality_data = {\n",
        "    \"Year\": years,\n",
        "    \"PM2.5_Concentration\": [18, 17, 16, 15, 14, 13, 12]  # Replace with actual data\n",
        "}\n",
        "\n",
        "# Generate health data (number of respiratory disease cases)\n",
        "health_data = {\n",
        "    \"Year\": years,\n",
        "    \"Respiratory_Disease_Cases\": [1200, 1100, 1000, 950, 900, 850, 800]  # Replace with actual data\n",
        "}\n",
        "\n",
        "# Generate environmental data (CO2 emissions in metric tons)\n",
        "environmental_data = {\n",
        "    \"Year\": years,\n",
        "    \"CO2_Emissions\": [50000, 49000, 48000, 47000, 46000, 45000, 44000]  # Replace with actual data\n",
        "}\n",
        "\n",
        "# Create DataFrames from the generated data\n",
        "ev_df = pd.DataFrame(ev_adoption_data)\n",
        "air_quality_df = pd.DataFrame(air_quality_data)\n",
        "health_df = pd.DataFrame(health_data)\n",
        "environmental_df = pd.DataFrame(environmental_data)\n",
        "\n",
        "# Merge DataFrames based on the \"Year\" column\n",
        "merged_data = pd.merge(ev_df, air_quality_df, on=\"Year\")\n",
        "merged_data = pd.merge(merged_data, health_df, on=\"Year\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b4ba61-e3eb-4d55-e511-21598c412e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected and saved 0 academic articles.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Define the URL for Google Scholar search\n",
        "url = \"https://scholar.google.com/\"\n",
        "query = \"XYZ\"  # Replace with your query\n",
        "years = \"2014-2024\"  # Limit results to the last 10 years\n",
        "num_articles = 1000  # Number of articles to collect\n",
        "\n",
        "# Initialize variables to track collected articles\n",
        "collected_articles = 0\n",
        "\n",
        "# Create a CSV file to store the collected data\n",
        "csv_filename = \"academic_articles.csv\"\n",
        "csv_file = open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\")\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow([\"Title\", \"Venue\", \"Year\", \"Authors\", \"Abstract\"])\n",
        "\n",
        "# Loop until the desired number of articles is collected\n",
        "while collected_articles < num_articles:\n",
        "    # Define query parameters for Google Scholar search\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"as_ylo\": years,\n",
        "        \"start\": collected_articles,\n",
        "    }\n",
        "\n",
        "    # Send an HTTP GET request to Google Scholar\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Find and extract information from search results\n",
        "        results = soup.find_all(\"div\", class_=\"gs_ri\")\n",
        "\n",
        "        for result in results:\n",
        "            # Extract data from the search result\n",
        "            title = result.find(\"h3\").get_text()\n",
        "            venue = result.find(\"div\", class_=\"gs_a\").get_text()\n",
        "            year = result.find(\"div\", class_=\"gs_a\").get_text()\n",
        "            authors = result.find(\"div\", class_=\"gs_a\").get_text()\n",
        "            abstract = result.find(\"div\", class_=\"gs_rs\").get_text()\n",
        "\n",
        "            # Write the data to the CSV file\n",
        "            csv_writer.writerow([title, venue, year, authors, abstract])\n",
        "\n",
        "            # Increment the collected_articles counter\n",
        "            collected_articles += 1\n",
        "\n",
        "            # Check if the desired number of articles is reached\n",
        "            if collected_articles >= num_articles:\n",
        "                break\n",
        "\n",
        "        # Check for pagination and exit if no more results are available\n",
        "        if not soup.find(\"button\", class_=\"gs_btnPR\"):\n",
        "            break\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to retrieve search results.\")\n",
        "        break\n",
        "\n",
        "# Close the CSV file\n",
        "csv_file.close()\n",
        "\n",
        "print(f\"Collected and saved {collected_articles} academic articles.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Set your Twitter API credentials\n",
        "consumer_key = 'tFF4XklOpDv6xUDyrkaG6RNpS'\n",
        "consumer_secret = 'wIWrag0SzZGBXO5ctE8ojuLCTYbXtm5Spp6MHF4ASbmPVchRkV'\n",
        "access_token = '1672728134238470145-DEmE2rBRv1Od7GhNP8ecDZ7kngRwvk'\n",
        "access_token_secret = 'ynY8PVDY3V32UT6xq3gM3MuxDujtrXWrjU6eaKvt89X5f'\n",
        "\n",
        "# Authenticate with Twitter\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create the API object\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Make an API request\n",
        "tweets = api.search(q='python', count=10)\n",
        "\n",
        "# Process the returned data\n",
        "for tweet in tweets:\n",
        "    print(tweet.text)  # Print the text of each tweet\n",
        "    print('---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "UjCSaytTqWjt",
        "outputId": "d2eccf0b-eced-4762-9b72-b3924bba7a4a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'API' object has no attribute 'search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b8b9a2938858>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Make an API request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Process the returned data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n",
        "https://myunt-my.sharepoint.com/:x:/g/personal/pramodgangula_my_unt_edu/ETTrkCh_zUdMl3vZegwKyWABZhURxjDz6ZdjWPTT-Q9mKA?e=jdmeXu\n",
        "i got this result from parsehub\n",
        "when using #texas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "\n",
        "Learning Experience:\n",
        "Overall, working on web scraping tasks provided a valuable learning experience in understanding how to extract data from various online sources programmatically. The key concepts that I found most beneficial include understanding HTML structure and tags, utilizing libraries like BeautifulSoup for parsing HTML content, and handling HTTP requests effectively using libraries such as requests. Learning about CSS selectors and XPath for navigating through HTML elements also proved to be valuable techniques for targeting specific data elements on web pages.\n",
        "\n",
        "Challenges Encountered:\n",
        "One of the challenges encountered during web scraping was dealing with dynamic content loaded via JavaScript. In such cases, the initial HTML response may not contain all the desired data, requiring additional techniques like using headless browsers or inspecting network requests to retrieve dynamically loaded content. Additionally, some websites enforce strict anti-scraping measures, such as CAPTCHA challenges or rate limiting, which can hinder data collection efforts. To overcome these challenges, it's important to explore alternative data sources or adjust scraping strategies to comply with website policies.\n",
        "\n",
        "Relevance to Your Field of Study:\n",
        "The ability to gather and analyze data from online sources is highly relevant across various fields of study, including but not limited to data science, social sciences, market research, and business intelligence. In my field of study, which focuses on natural language processing and machine learning, web scraping enables the collection of text data from diverse sources such as social media, news articles, and forums. This data can then be used for tasks like sentiment analysis, topic modeling, and training machine learning models. Additionally, web scraping facilitates research by providing access to up-to-date information and large-scale datasets for analysis, thereby enhancing the depth and scope of research outcomes.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}